{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pravachanpatra/Documents/PYTHON/AI_ML_DL/Stock_Price_Predictor/venv/bin/python\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>vw_avr_price</th>\n",
       "      <th>open_price</th>\n",
       "      <th>close_price</th>\n",
       "      <th>high_price</th>\n",
       "      <th>low_price</th>\n",
       "      <th>num_transactions</th>\n",
       "      <th>datetime</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>volume_sma_5</th>\n",
       "      <th>volume_change</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>vol_5</th>\n",
       "      <th>daily_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.669698e+07</td>\n",
       "      <td>164.743200</td>\n",
       "      <td>163.210000</td>\n",
       "      <td>165.35</td>\n",
       "      <td>165.850000</td>\n",
       "      <td>163.00</td>\n",
       "      <td>491310.000000</td>\n",
       "      <td>2022-08-02 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.694134e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.318000</td>\n",
       "      <td>169.103000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>0.071554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.669698e+07</td>\n",
       "      <td>164.743200</td>\n",
       "      <td>163.210000</td>\n",
       "      <td>165.35</td>\n",
       "      <td>165.850000</td>\n",
       "      <td>163.00</td>\n",
       "      <td>491310.000000</td>\n",
       "      <td>2022-08-03 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.694134e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.318000</td>\n",
       "      <td>169.103000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>0.071554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.669698e+07</td>\n",
       "      <td>164.743200</td>\n",
       "      <td>163.210000</td>\n",
       "      <td>165.35</td>\n",
       "      <td>165.850000</td>\n",
       "      <td>163.00</td>\n",
       "      <td>491310.000000</td>\n",
       "      <td>2022-08-04 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.694134e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.318000</td>\n",
       "      <td>169.103000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>0.071554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.669698e+07</td>\n",
       "      <td>164.743200</td>\n",
       "      <td>163.210000</td>\n",
       "      <td>165.35</td>\n",
       "      <td>165.850000</td>\n",
       "      <td>163.00</td>\n",
       "      <td>491310.000000</td>\n",
       "      <td>2022-08-05 04:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.694134e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.318000</td>\n",
       "      <td>169.103000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>165.350000</td>\n",
       "      <td>0.071554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.791877e+07</td>\n",
       "      <td>165.126767</td>\n",
       "      <td>164.263333</td>\n",
       "      <td>165.19</td>\n",
       "      <td>166.503333</td>\n",
       "      <td>163.40</td>\n",
       "      <td>507545.666667</td>\n",
       "      <td>2022-08-06 00:00:00</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>5.694134e+07</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>165.318000</td>\n",
       "      <td>169.103000</td>\n",
       "      <td>165.296667</td>\n",
       "      <td>165.334762</td>\n",
       "      <td>0.071554</td>\n",
       "      <td>-0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>3.515373e+07</td>\n",
       "      <td>218.131900</td>\n",
       "      <td>216.960000</td>\n",
       "      <td>218.24</td>\n",
       "      <td>219.300000</td>\n",
       "      <td>215.75</td>\n",
       "      <td>604680.000000</td>\n",
       "      <td>2024-07-29 04:00:00</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>4.008290e+07</td>\n",
       "      <td>-0.042438</td>\n",
       "      <td>217.978000</td>\n",
       "      <td>225.414500</td>\n",
       "      <td>218.726415</td>\n",
       "      <td>221.994869</td>\n",
       "      <td>0.292077</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>4.068162e+07</td>\n",
       "      <td>218.405900</td>\n",
       "      <td>219.190000</td>\n",
       "      <td>218.80</td>\n",
       "      <td>220.325000</td>\n",
       "      <td>216.12</td>\n",
       "      <td>584305.000000</td>\n",
       "      <td>2024-07-30 04:00:00</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>3.812887e+07</td>\n",
       "      <td>0.157249</td>\n",
       "      <td>218.240000</td>\n",
       "      <td>224.705500</td>\n",
       "      <td>218.750943</td>\n",
       "      <td>221.690596</td>\n",
       "      <td>0.329983</td>\n",
       "      <td>0.002566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>4.842297e+07</td>\n",
       "      <td>222.344100</td>\n",
       "      <td>221.440000</td>\n",
       "      <td>222.08</td>\n",
       "      <td>223.820000</td>\n",
       "      <td>220.63</td>\n",
       "      <td>668833.000000</td>\n",
       "      <td>2024-07-31 04:00:00</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>3.984794e+07</td>\n",
       "      <td>0.190291</td>\n",
       "      <td>219.064000</td>\n",
       "      <td>224.431000</td>\n",
       "      <td>219.860629</td>\n",
       "      <td>221.727682</td>\n",
       "      <td>1.710839</td>\n",
       "      <td>0.014991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>6.112524e+07</td>\n",
       "      <td>219.477300</td>\n",
       "      <td>224.370000</td>\n",
       "      <td>218.36</td>\n",
       "      <td>224.480000</td>\n",
       "      <td>217.02</td>\n",
       "      <td>876046.000000</td>\n",
       "      <td>2024-08-01 04:00:00</td>\n",
       "      <td>-3.720000</td>\n",
       "      <td>4.441905e+07</td>\n",
       "      <td>0.262319</td>\n",
       "      <td>219.125333</td>\n",
       "      <td>223.822000</td>\n",
       "      <td>219.360419</td>\n",
       "      <td>221.406950</td>\n",
       "      <td>1.670571</td>\n",
       "      <td>-0.016751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>6.112524e+07</td>\n",
       "      <td>219.477300</td>\n",
       "      <td>224.370000</td>\n",
       "      <td>218.36</td>\n",
       "      <td>224.480000</td>\n",
       "      <td>217.02</td>\n",
       "      <td>876046.000000</td>\n",
       "      <td>2024-08-02 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.930176e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>219.168000</td>\n",
       "      <td>223.148667</td>\n",
       "      <td>219.026946</td>\n",
       "      <td>221.116765</td>\n",
       "      <td>1.641804</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           volume  vw_avr_price  open_price  close_price  high_price  \\\n",
       "0    5.669698e+07    164.743200  163.210000       165.35  165.850000   \n",
       "1    5.669698e+07    164.743200  163.210000       165.35  165.850000   \n",
       "2    5.669698e+07    164.743200  163.210000       165.35  165.850000   \n",
       "3    5.669698e+07    164.743200  163.210000       165.35  165.850000   \n",
       "4    5.791877e+07    165.126767  164.263333       165.19  166.503333   \n",
       "..            ...           ...         ...          ...         ...   \n",
       "727  3.515373e+07    218.131900  216.960000       218.24  219.300000   \n",
       "728  4.068162e+07    218.405900  219.190000       218.80  220.325000   \n",
       "729  4.842297e+07    222.344100  221.440000       222.08  223.820000   \n",
       "730  6.112524e+07    219.477300  224.370000       218.36  224.480000   \n",
       "731  6.112524e+07    219.477300  224.370000       218.36  224.480000   \n",
       "\n",
       "     low_price  num_transactions            datetime  price_diff  \\\n",
       "0       163.00     491310.000000 2022-08-02 00:00:00    0.000000   \n",
       "1       163.00     491310.000000 2022-08-03 00:00:00    0.000000   \n",
       "2       163.00     491310.000000 2022-08-04 00:00:00    0.000000   \n",
       "3       163.00     491310.000000 2022-08-05 04:00:00    0.000000   \n",
       "4       163.40     507545.666667 2022-08-06 00:00:00   -0.160000   \n",
       "..         ...               ...                 ...         ...   \n",
       "727     215.75     604680.000000 2024-07-29 04:00:00    0.093333   \n",
       "728     216.12     584305.000000 2024-07-30 04:00:00    0.560000   \n",
       "729     220.63     668833.000000 2024-07-31 04:00:00    3.280000   \n",
       "730     217.02     876046.000000 2024-08-01 04:00:00   -3.720000   \n",
       "731     217.02     876046.000000 2024-08-02 00:00:00    0.000000   \n",
       "\n",
       "     volume_sma_5  volume_change       SMA_5      SMA_20       EMA_5  \\\n",
       "0    5.694134e+07       0.000000  165.318000  169.103000  165.350000   \n",
       "1    5.694134e+07       0.000000  165.318000  169.103000  165.350000   \n",
       "2    5.694134e+07       0.000000  165.318000  169.103000  165.350000   \n",
       "3    5.694134e+07       0.000000  165.318000  169.103000  165.350000   \n",
       "4    5.694134e+07       0.021549  165.318000  169.103000  165.296667   \n",
       "..            ...            ...         ...         ...         ...   \n",
       "727  4.008290e+07      -0.042438  217.978000  225.414500  218.726415   \n",
       "728  3.812887e+07       0.157249  218.240000  224.705500  218.750943   \n",
       "729  3.984794e+07       0.190291  219.064000  224.431000  219.860629   \n",
       "730  4.441905e+07       0.262319  219.125333  223.822000  219.360419   \n",
       "731  4.930176e+07       0.000000  219.168000  223.148667  219.026946   \n",
       "\n",
       "         EMA_20     vol_5  daily_return  \n",
       "0    165.350000  0.071554      0.000000  \n",
       "1    165.350000  0.071554      0.000000  \n",
       "2    165.350000  0.071554      0.000000  \n",
       "3    165.350000  0.071554      0.000000  \n",
       "4    165.334762  0.071554     -0.000968  \n",
       "..          ...       ...           ...  \n",
       "727  221.994869  0.292077      0.000428  \n",
       "728  221.690596  0.329983      0.002566  \n",
       "729  221.727682  1.710839      0.014991  \n",
       "730  221.406950  1.670571     -0.016751  \n",
       "731  221.116765  1.641804      0.000000  \n",
       "\n",
       "[732 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "# Verify kernal path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# time series dataframe\n",
    "ts_prices = pd.read_parquet(\"../data/transformed/validated_prices_2022-08-02-2024-08-02.parquet\") # reads in parquet-file in specified path and returns its data in a dataframe\n",
    "ts_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13),\n",
       " (1, 14),\n",
       " (2, 15),\n",
       " (3, 16),\n",
       " (4, 17),\n",
       " (5, 18),\n",
       " (6, 19),\n",
       " (7, 20),\n",
       " (8, 21),\n",
       " (9, 22)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cutoff_indicies(data, n_previous_days, step_size):\n",
    "    stop_position = len(data)-1\n",
    "\n",
    "    first_indx = 0\n",
    "    last_indx = n_previous_days+1  # target index\n",
    "    indicies = []\n",
    "\n",
    "    while last_indx <= stop_position:\n",
    "        indicies.append((first_indx, last_indx))\n",
    "\n",
    "        first_indx += step_size\n",
    "        last_indx += step_size\n",
    "    \n",
    "    return indicies\n",
    "\n",
    "n_previous_days = 12  # make sure this constant across all uses\n",
    "step_size = 1\n",
    "indicies = get_cutoff_indicies(ts_prices, n_previous_days, step_size)\n",
    "indicies[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 13), (1, 14), (2, 15), (3, 16), (4, 17), (5, 18), (6, 19), (7, 20), (8, 21), (9, 22), (10, 23), (11, 24), (12, 25), (13, 26), (14, 27), (15, 28), (16, 29), (17, 30), (18, 31), (19, 32), (20, 33), (21, 34), (22, 35), (23, 36), (24, 37), (25, 38), (26, 39), (27, 40), (28, 41), (29, 42), (30, 43), (31, 44), (32, 45), (33, 46), (34, 47), (35, 48), (36, 49), (37, 50), (38, 51), (39, 52), (40, 53), (41, 54), (42, 55), (43, 56), (44, 57), (45, 58), (46, 59), (47, 60), (48, 61), (49, 62), (50, 63), (51, 64), (52, 65), (53, 66), (54, 67), (55, 68), (56, 69), (57, 70), (58, 71), (59, 72), (60, 73), (61, 74), (62, 75), (63, 76), (64, 77), (65, 78), (66, 79), (67, 80), (68, 81), (69, 82), (70, 83), (71, 84), (72, 85), (73, 86), (74, 87), (75, 88), (76, 89), (77, 90), (78, 91), (79, 92), (80, 93), (81, 94), (82, 95), (83, 96), (84, 97), (85, 98), (86, 99), (87, 100), (88, 101), (89, 102), (90, 103), (91, 104), (92, 105), (93, 106), (94, 107), (95, 108), (96, 109), (97, 110), (98, 111), (99, 112), (100, 113), (101, 114), (102, 115), (103, 116), (104, 117), (105, 118), (106, 119), (107, 120), (108, 121), (109, 122), (110, 123), (111, 124), (112, 125), (113, 126), (114, 127), (115, 128), (116, 129), (117, 130), (118, 131), (119, 132), (120, 133), (121, 134), (122, 135), (123, 136), (124, 137), (125, 138), (126, 139), (127, 140), (128, 141), (129, 142), (130, 143), (131, 144), (132, 145), (133, 146), (134, 147), (135, 148), (136, 149), (137, 150), (138, 151), (139, 152), (140, 153), (141, 154), (142, 155), (143, 156), (144, 157), (145, 158), (146, 159), (147, 160), (148, 161), (149, 162), (150, 163), (151, 164), (152, 165), (153, 166), (154, 167), (155, 168), (156, 169), (157, 170), (158, 171), (159, 172), (160, 173), (161, 174), (162, 175), (163, 176), (164, 177), (165, 178), (166, 179), (167, 180), (168, 181), (169, 182), (170, 183), (171, 184), (172, 185), (173, 186), (174, 187), (175, 188), (176, 189), (177, 190), (178, 191), (179, 192), (180, 193), (181, 194), (182, 195), (183, 196), (184, 197), (185, 198), (186, 199), (187, 200), (188, 201), (189, 202), (190, 203), (191, 204), (192, 205), (193, 206), (194, 207), (195, 208), (196, 209), (197, 210), (198, 211), (199, 212), (200, 213), (201, 214), (202, 215), (203, 216), (204, 217), (205, 218), (206, 219), (207, 220), (208, 221), (209, 222), (210, 223), (211, 224), (212, 225), (213, 226), (214, 227), (215, 228), (216, 229), (217, 230), (218, 231), (219, 232), (220, 233), (221, 234), (222, 235), (223, 236), (224, 237), (225, 238), (226, 239), (227, 240), (228, 241), (229, 242), (230, 243), (231, 244), (232, 245), (233, 246), (234, 247), (235, 248), (236, 249), (237, 250), (238, 251), (239, 252), (240, 253), (241, 254), (242, 255), (243, 256), (244, 257), (245, 258), (246, 259), (247, 260), (248, 261), (249, 262), (250, 263), (251, 264), (252, 265), (253, 266), (254, 267), (255, 268), (256, 269), (257, 270), (258, 271), (259, 272), (260, 273), (261, 274), (262, 275), (263, 276), (264, 277), (265, 278), (266, 279), (267, 280), (268, 281), (269, 282), (270, 283), (271, 284), (272, 285), (273, 286), (274, 287), (275, 288), (276, 289), (277, 290), (278, 291), (279, 292), (280, 293), (281, 294), (282, 295), (283, 296), (284, 297), (285, 298), (286, 299), (287, 300), (288, 301), (289, 302), (290, 303), (291, 304), (292, 305), (293, 306), (294, 307), (295, 308), (296, 309), (297, 310), (298, 311), (299, 312), (300, 313), (301, 314), (302, 315), (303, 316), (304, 317), (305, 318), (306, 319), (307, 320), (308, 321), (309, 322), (310, 323), (311, 324), (312, 325), (313, 326), (314, 327), (315, 328), (316, 329), (317, 330), (318, 331), (319, 332), (320, 333), (321, 334), (322, 335), (323, 336), (324, 337), (325, 338), (326, 339), (327, 340), (328, 341), (329, 342), (330, 343), (331, 344), (332, 345), (333, 346), (334, 347), (335, 348), (336, 349), (337, 350), (338, 351), (339, 352), (340, 353), (341, 354), (342, 355), (343, 356), (344, 357), (345, 358), (346, 359), (347, 360), (348, 361), (349, 362), (350, 363), (351, 364), (352, 365), (353, 366), (354, 367), (355, 368), (356, 369), (357, 370), (358, 371), (359, 372), (360, 373), (361, 374), (362, 375), (363, 376), (364, 377), (365, 378), (366, 379), (367, 380), (368, 381), (369, 382), (370, 383), (371, 384), (372, 385), (373, 386), (374, 387), (375, 388), (376, 389), (377, 390), (378, 391), (379, 392), (380, 393), (381, 394), (382, 395), (383, 396), (384, 397), (385, 398), (386, 399), (387, 400), (388, 401), (389, 402), (390, 403), (391, 404), (392, 405), (393, 406), (394, 407), (395, 408), (396, 409), (397, 410), (398, 411), (399, 412), (400, 413), (401, 414), (402, 415), (403, 416), (404, 417), (405, 418), (406, 419), (407, 420), (408, 421), (409, 422), (410, 423), (411, 424), (412, 425), (413, 426), (414, 427), (415, 428), (416, 429), (417, 430), (418, 431), (419, 432), (420, 433), (421, 434), (422, 435), (423, 436), (424, 437), (425, 438), (426, 439), (427, 440), (428, 441), (429, 442), (430, 443), (431, 444), (432, 445), (433, 446), (434, 447), (435, 448), (436, 449), (437, 450), (438, 451), (439, 452), (440, 453), (441, 454), (442, 455), (443, 456), (444, 457), (445, 458), (446, 459), (447, 460), (448, 461), (449, 462), (450, 463), (451, 464), (452, 465), (453, 466), (454, 467), (455, 468), (456, 469), (457, 470), (458, 471), (459, 472), (460, 473), (461, 474), (462, 475), (463, 476), (464, 477), (465, 478), (466, 479), (467, 480), (468, 481), (469, 482), (470, 483), (471, 484), (472, 485), (473, 486), (474, 487), (475, 488), (476, 489), (477, 490), (478, 491), (479, 492), (480, 493), (481, 494), (482, 495), (483, 496), (484, 497), (485, 498), (486, 499), (487, 500), (488, 501), (489, 502), (490, 503), (491, 504), (492, 505), (493, 506), (494, 507), (495, 508), (496, 509), (497, 510), (498, 511), (499, 512), (500, 513), (501, 514), (502, 515), (503, 516), (504, 517), (505, 518), (506, 519), (507, 520), (508, 521), (509, 522), (510, 523), (511, 524), (512, 525), (513, 526), (514, 527), (515, 528), (516, 529), (517, 530), (518, 531), (519, 532), (520, 533), (521, 534), (522, 535), (523, 536), (524, 537), (525, 538), (526, 539), (527, 540), (528, 541), (529, 542), (530, 543), (531, 544), (532, 545), (533, 546), (534, 547), (535, 548), (536, 549), (537, 550), (538, 551), (539, 552), (540, 553), (541, 554), (542, 555), (543, 556), (544, 557), (545, 558), (546, 559), (547, 560), (548, 561), (549, 562), (550, 563), (551, 564), (552, 565), (553, 566), (554, 567), (555, 568), (556, 569), (557, 570), (558, 571), (559, 572), (560, 573), (561, 574), (562, 575), (563, 576), (564, 577), (565, 578), (566, 579), (567, 580), (568, 581), (569, 582), (570, 583), (571, 584), (572, 585), (573, 586), (574, 587), (575, 588), (576, 589), (577, 590), (578, 591), (579, 592), (580, 593), (581, 594), (582, 595), (583, 596), (584, 597), (585, 598), (586, 599), (587, 600), (588, 601), (589, 602), (590, 603), (591, 604), (592, 605), (593, 606), (594, 607), (595, 608), (596, 609), (597, 610), (598, 611), (599, 612), (600, 613), (601, 614), (602, 615), (603, 616), (604, 617), (605, 618), (606, 619), (607, 620), (608, 621), (609, 622), (610, 623), (611, 624), (612, 625), (613, 626), (614, 627), (615, 628), (616, 629), (617, 630), (618, 631), (619, 632), (620, 633), (621, 634), (622, 635), (623, 636), (624, 637), (625, 638), (626, 639), (627, 640), (628, 641), (629, 642), (630, 643), (631, 644), (632, 645), (633, 646), (634, 647), (635, 648), (636, 649), (637, 650), (638, 651), (639, 652), (640, 653), (641, 654), (642, 655), (643, 656), (644, 657), (645, 658), (646, 659), (647, 660), (648, 661), (649, 662), (650, 663), (651, 664), (652, 665), (653, 666), (654, 667), (655, 668), (656, 669), (657, 670), (658, 671), (659, 672), (660, 673), (661, 674), (662, 675), (663, 676), (664, 677), (665, 678), (666, 679), (667, 680), (668, 681), (669, 682), (670, 683), (671, 684), (672, 685), (673, 686), (674, 687), (675, 688), (676, 689), (677, 690), (678, 691), (679, 692), (680, 693), (681, 694), (682, 695), (683, 696), (684, 697), (685, 698), (686, 699), (687, 700), (688, 701), (689, 702), (690, 703), (691, 704), (692, 705), (693, 706), (694, 707), (695, 708), (696, 709), (697, 710), (698, 711), (699, 712), (700, 713), (701, 714), (702, 715), (703, 716), (704, 717), (705, 718), (706, 719), (707, 720), (708, 721), (709, 722), (710, 723), (711, 724), (712, 725), (713, 726), (714, 727), (715, 728), (716, 729), (717, 730), (718, 731)]\n",
      "Number of sequences/examples: 718\n",
      "Last val: 172.82666666666665\n",
      "Target val: 173.19\n",
      "\n",
      "Confirm number of examples: True\n",
      "Num-features-in-each-day: 16\n",
      "N-previous-days: 12\n",
      "Step-size: 1\n",
      "Features shape: (718, 13, 16)\n",
      "Targets shape: (718,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 6.8039384e+07,  1.7107539e+02,  1.6982001e+02, ...,\n",
       "          1.6647890e+02,  3.0757976e+00,  2.1425605e-02],\n",
       "        [ 6.3390152e+07,  1.7159207e+02,  1.7038667e+02, ...,\n",
       "          1.6704884e+02,  3.0654938e+00,  2.1111756e-03],\n",
       "        [ 5.8740924e+07,  1.7210873e+02,  1.7095334e+02, ...,\n",
       "          1.6759911e+02,  2.0051730e+00,  2.1067280e-03]],\n",
       "\n",
       "       [[ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 6.3390152e+07,  1.7159207e+02,  1.7038667e+02, ...,\n",
       "          1.6704884e+02,  3.0654938e+00,  2.1111756e-03],\n",
       "        [ 5.8740924e+07,  1.7210873e+02,  1.7095334e+02, ...,\n",
       "          1.6759911e+02,  2.0051730e+00,  2.1067280e-03],\n",
       "        [ 5.4091696e+07,  1.7262540e+02,  1.7152000e+02, ...,\n",
       "          1.6813158e+02,  1.9020565e+00,  2.1022991e-03]],\n",
       "\n",
       "       [[ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.6696984e+07,  1.6474319e+02,  1.6321001e+02, ...,\n",
       "          1.6535001e+02,  7.1554177e-02,  0.0000000e+00],\n",
       "        [ 5.7918768e+07,  1.6512677e+02,  1.6426334e+02, ...,\n",
       "          1.6533476e+02,  7.1554177e-02, -9.6764439e-04],\n",
       "        ...,\n",
       "        [ 5.8740924e+07,  1.7210873e+02,  1.7095334e+02, ...,\n",
       "          1.6759911e+02,  2.0051730e+00,  2.1067280e-03],\n",
       "        [ 5.4091696e+07,  1.7262540e+02,  1.7152000e+02, ...,\n",
       "          1.6813158e+02,  1.9020565e+00,  2.1022991e-03],\n",
       "        [ 5.6377048e+07,  1.7274271e+02,  1.7278000e+02, ...,\n",
       "          1.6859810e+02,  4.4120163e-01, -9.2384085e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.5878904e+07,  2.2883960e+02,  2.2945000e+02, ...,\n",
       "          2.2596083e+02,  2.3910267e+00, -2.5295971e-02],\n",
       "        [ 6.4346032e+07,  2.2518100e+02,  2.3028000e+02, ...,\n",
       "          2.2579123e+02,  4.5164819e+00, -2.0534778e-02],\n",
       "        [ 4.8020040e+07,  2.2473560e+02,  2.2482001e+02, ...,\n",
       "          2.2565015e+02,  5.1906090e+00,  5.7989114e-04],\n",
       "        ...,\n",
       "        [ 3.8269672e+07,  2.1797676e+02,  2.1812000e+02, ...,\n",
       "          2.2283679e+02,  3.1522350e+00,  4.2821313e-04],\n",
       "        [ 3.6711700e+07,  2.1805434e+02,  2.1753999e+02, ...,\n",
       "          2.2239012e+02,  3.7763593e-01,  4.2802983e-04],\n",
       "        [ 3.5153728e+07,  2.1813190e+02,  2.1696001e+02, ...,\n",
       "          2.2199487e+02,  2.9207686e-01,  4.2784671e-04]],\n",
       "\n",
       "       [[ 6.4346032e+07,  2.2518100e+02,  2.3028000e+02, ...,\n",
       "          2.2579123e+02,  4.5164819e+00, -2.0534778e-02],\n",
       "        [ 4.8020040e+07,  2.2473560e+02,  2.2482001e+02, ...,\n",
       "          2.2565015e+02,  5.1906090e+00,  5.7989114e-04],\n",
       "        [ 4.6999404e+07,  2.2482990e+02,  2.2555000e+02, ...,\n",
       "          2.2551141e+02,  4.6735950e+00, -5.2011351e-04],\n",
       "        ...,\n",
       "        [ 3.6711700e+07,  2.1805434e+02,  2.1753999e+02, ...,\n",
       "          2.2239012e+02,  3.7763593e-01,  4.2802983e-04],\n",
       "        [ 3.5153728e+07,  2.1813190e+02,  2.1696001e+02, ...,\n",
       "          2.2199487e+02,  2.9207686e-01,  4.2784671e-04],\n",
       "        [ 4.0681624e+07,  2.1840590e+02,  2.1919000e+02, ...,\n",
       "          2.2169060e+02,  3.2998317e-01,  2.5659825e-03]],\n",
       "\n",
       "       [[ 4.8020040e+07,  2.2473560e+02,  2.2482001e+02, ...,\n",
       "          2.2565015e+02,  5.1906090e+00,  5.7989114e-04],\n",
       "        [ 4.6999404e+07,  2.2482990e+02,  2.2555000e+02, ...,\n",
       "          2.2551141e+02,  4.6735950e+00, -5.2011351e-04],\n",
       "        [ 4.5978772e+07,  2.2492419e+02,  2.2628000e+02, ...,\n",
       "          2.2537477e+02,  2.0990615e+00, -5.2038417e-04],\n",
       "        ...,\n",
       "        [ 3.5153728e+07,  2.1813190e+02,  2.1696001e+02, ...,\n",
       "          2.2199487e+02,  2.9207686e-01,  4.2784671e-04],\n",
       "        [ 4.0681624e+07,  2.1840590e+02,  2.1919000e+02, ...,\n",
       "          2.2169060e+02,  3.2998317e-01,  2.5659825e-03],\n",
       "        [ 4.8422976e+07,  2.2234410e+02,  2.2144000e+02, ...,\n",
       "          2.2172768e+02,  1.7108387e+00,  1.4990860e-02]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (number_of_sequences, sequence_length, number_of_features)\n",
    "# each sequence is an example, seq_len is number of days in each example, features is values per day\n",
    "# number of cutoff indicies is each example\n",
    "# for each cutoff indicies-pair get those days, create a matrix for each day add those days features as a row\n",
    "# then add this matrix for this sequence to the total matrix as a single example\n",
    "def transform_ts_data_into_features_target(ts_prices, n_previous_days, step_size):\n",
    "    indicies = get_cutoff_indicies(ts_prices, n_previous_days, step_size)\n",
    "    indicies = indicies[:len(indicies)-1]  # exclude last pair\n",
    "    num_seqs = len(indicies)\n",
    "    print(f\"Number of sequences/examples: {num_seqs}\")\n",
    "\n",
    "    features = []  \n",
    "    targets = []\n",
    "\n",
    "    for pair in indicies:\n",
    "        start_indx = pair[0]  # start-index of day-row to end-index of day-row, all days in between this range is a sequence/example\n",
    "        end_indx = pair[1]  # target indx of row\n",
    "\n",
    "        sequence_df = ts_prices.iloc[start_indx: end_indx]  # get day-rows from start-indx to just before end-indx, each element is a day in current-sequence-day-example\n",
    "\n",
    "        cur_seq_days_matrix = []  # each row is the values of features for each day in cur-sequence-example\n",
    "        for _, day_row in sequence_df.iterrows():\n",
    "            \n",
    "            values = list(day_row.drop(labels=\"datetime\").values)\n",
    "            cur_seq_days_matrix.append(values)\n",
    "\n",
    "        features.append(cur_seq_days_matrix)\n",
    "        target_value = ts_prices.iloc[end_indx][\"close_price\"]   # target row is end-indx close-price\n",
    "        targets.append(target_value)\n",
    "\n",
    "        if pair == indicies[0]: # confirm last-val of seq is not same as the target next value\n",
    "            print(f\"Last val: {sequence_df.iloc[-1][\"close_price\"]}\")\n",
    "            print(f\"Target val: {target_value}\")\n",
    "\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "    return features, targets\n",
    "\n",
    "print(indicies)\n",
    "features, targets = transform_ts_data_into_features_target(ts_prices, n_previous_days, step_size)\n",
    "print(f\"\\nConfirm number of examples: {len(features) == len(targets)}\")\n",
    "print(f\"Num-features-in-each-day: {len(ts_prices.columns)-1}\")\n",
    "print(f\"N-previous-days: {n_previous_days}\")\n",
    "print(f\"Step-size: {step_size}\")\n",
    "print(f'Features shape: {features.shape}')  # Should be (number_of_examples, n_previous_days, n_features)\n",
    "print(f'Targets shape: {targets.shape}')    # Should be (number_of_examples,)\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 27380.8496 - val_loss: 33454.0781\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 24665.2969 - val_loss: 32442.2227\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 23719.2246 - val_loss: 31719.1211\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 23282.9473 - val_loss: 31036.6074\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 22562.7578 - val_loss: 30381.8047\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 21940.9414 - val_loss: 29747.9199\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 21327.9980 - val_loss: 29131.3535\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 20856.7695 - val_loss: 28528.5938\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 20626.6191 - val_loss: 27938.0547\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 19774.4258 - val_loss: 27362.1250\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - loss: 19329.0664 - val_loss: 26796.6152\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 18754.0312 - val_loss: 26243.2344\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 19032.5938 - val_loss: 25696.2188\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 18074.9316 - val_loss: 25162.8496\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 17618.8105 - val_loss: 24638.4629\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 17269.0977 - val_loss: 24124.4883\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 16511.4629 - val_loss: 23621.2422\n",
      "Epoch 18/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 16235.9824 - val_loss: 23124.6523\n",
      "Epoch 19/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 15630.8164 - val_loss: 22637.4434\n",
      "Epoch 20/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 15334.2236 - val_loss: 22158.9746\n",
      "Epoch 21/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 14937.8750 - val_loss: 21689.9609\n",
      "Epoch 22/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 14955.8242 - val_loss: 21228.2480\n",
      "Epoch 23/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 14354.8379 - val_loss: 20775.7852\n",
      "Epoch 24/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 13813.3018 - val_loss: 20331.4375\n",
      "Epoch 25/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 13716.5908 - val_loss: 19895.0234\n",
      "Epoch 26/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 12838.7266 - val_loss: 19468.5195\n",
      "Epoch 27/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 12911.1895 - val_loss: 19047.1699\n",
      "Epoch 28/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 12342.9609 - val_loss: 18634.5586\n",
      "Epoch 29/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 12189.5957 - val_loss: 18229.0664\n",
      "Epoch 30/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 11778.8877 - val_loss: 17830.3770\n",
      "Epoch 31/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 11716.5342 - val_loss: 17439.7910\n",
      "Epoch 32/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 11509.1006 - val_loss: 17056.8125\n",
      "Epoch 33/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - loss: 10844.3047 - val_loss: 16681.5449\n",
      "Epoch 34/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 10617.7783 - val_loss: 16312.8799\n",
      "Epoch 35/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 10170.0146 - val_loss: 15951.4824\n",
      "Epoch 36/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 10279.3125 - val_loss: 15596.0352\n",
      "Epoch 37/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 9996.2695 - val_loss: 15247.3389\n",
      "Epoch 38/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 9586.3359 - val_loss: 14906.1768\n",
      "Epoch 39/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 9330.9473 - val_loss: 14571.6182\n",
      "Epoch 40/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 9269.2100 - val_loss: 14242.6143\n",
      "Epoch 41/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 8708.8242 - val_loss: 13921.3350\n",
      "Epoch 42/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 8428.8262 - val_loss: 13605.9287\n",
      "Epoch 43/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - loss: 8173.8550 - val_loss: 13297.3467\n",
      "Epoch 44/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 7902.7798 - val_loss: 12994.3105\n",
      "Epoch 45/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - loss: 7695.8882 - val_loss: 12697.0645\n",
      "Epoch 46/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 7486.1304 - val_loss: 12405.9824\n",
      "Epoch 47/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 7200.8789 - val_loss: 12121.2432\n",
      "Epoch 48/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 7140.7295 - val_loss: 11840.5225\n",
      "Epoch 49/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 6829.9482 - val_loss: 11567.0869\n",
      "Epoch 50/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 6665.8071 - val_loss: 11298.5137\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "[173.19 173.03 174.55 174.15 171.52]\n",
      "[[89.43906 ]\n",
      " [89.43925 ]\n",
      " [89.43911 ]\n",
      " [89.438835]\n",
      " [89.43927 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "X = features\n",
    "Y = targets\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=10, return_sequences=True, input_shape=(X.shape[1], X.shape[2]))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(1))  # Output layer for a single prediction\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X, Y, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Example: print the first few predictions\n",
    "print(targets[:5])\n",
    "print(predictions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pravachanpatra/Documents/PYTHON/AI_ML_DL/Stock_Price_Predictor/venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 27711.4941 - val_loss: 36395.0547\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27394.4277 - val_loss: 35758.0156\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26830.9531 - val_loss: 35246.4805\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26480.9668 - val_loss: 34804.0859\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26023.2676 - val_loss: 34452.2070\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25668.1074 - val_loss: 34177.2383\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25640.0254 - val_loss: 33931.6680\n",
      "Epoch 8/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25316.8711 - val_loss: 33698.7031\n",
      "Epoch 9/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24797.6875 - val_loss: 33472.7188\n",
      "Epoch 10/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24810.4629 - val_loss: 33251.2500\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "[173.19 173.03 174.55 174.15 171.52]\n",
      "[[12.447848]\n",
      " [12.447848]\n",
      " [12.447848]\n",
      " [12.423939]\n",
      " [12.447652]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=10, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
    "model.add(SimpleRNN(units=30))  # Second RNN layer\n",
    "model.add(Dense(1))  # Output layer for a single prediction\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X, Y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Example: print the first few predictions\n",
    "print(targets[:5])\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 140112696442880.0000 - val_loss: 54672329015296.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 40595766640640.0000 - val_loss: 9893673172992.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 19285615312896.0000 - val_loss: 3162523828224.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11363340517376.0000 - val_loss: 2291676741632.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8566870638592.0000 - val_loss: 1665333067776.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6914778857472.0000 - val_loss: 1291770527744.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5075327516672.0000 - val_loss: 1092190011392.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4810976264192.0000 - val_loss: 1261350420480.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3373449347072.0000 - val_loss: 1287489191936.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2837832531968.0000 - val_loss: 956124430336.0000\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step\n",
      "[173.19 173.03 174.55 174.15 171.52]\n",
      "[[-285468.75]\n",
      " [ -77719.5 ]\n",
      " [-474398.25]\n",
      " [-665406.  ]\n",
      " [-317124.25]]\n"
     ]
    }
   ],
   "source": [
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_shape=(X_flat.shape[1],)))  # Input shape is (sequence_length * number_of_features,)\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(1))  # Output layer for a single prediction\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_flat, Y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict(X_flat)\n",
    "\n",
    "print(targets[:5])\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7629116531530507\n",
      "Predictions: [173.20205139 172.99509949 173.702052   175.07864731 172.23541946]\n",
      "Actual values: [173.19 173.03 174.55 174.15 171.52]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "num_samples, sequence_length, num_features = X.shape\n",
    "X_train_flattened = X.reshape(num_samples, sequence_length * num_features)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_flattened, Y)\n",
    "\n",
    "y_pred = model.predict(X_train_flattened)\n",
    "\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "# Print the first few predictions and actual values\n",
    "print(\"Predictions:\", y_pred[:5])\n",
    "print(\"Actual values:\", Y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pravachanpatra/Documents/PYTHON/AI_ML_DL/Stock_Price_Predictor/venv/lib/python3.12/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# cannot combine because 3d:\n",
    "# tabular_data = pd.DataFrame(features)\n",
    "# tabular_data[\"target_close_price_next_day\"] = pd.DataFrame(targets)\n",
    "# tabular_data\n",
    "# # each column is a dayx-featurey\n",
    "\n",
    "# dont save because its 3d: save features/targets tabular data\n",
    "# tabular_data.to_parquet(\"../data/transformed/tabular_prices_2022-08-02-2024-08-02.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9815958805094114\n",
      "221\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 0.7365136091476493\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
